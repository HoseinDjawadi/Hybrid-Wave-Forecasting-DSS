{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlScDQet6aoPiT+j2bC1WE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEek47umAVqu","executionInfo":{"status":"ok","timestamp":1753644488253,"user_tz":-210,"elapsed":141122,"user":{"displayName":"Mo Ho Seyed Djawadi","userId":"09814257735504616511"}},"outputId":"b952577e-1c71-443a-f30b-16af2c24f065"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Google Drive mounted successfully.\n","Output directory created at: /content/drive/My Drive/Paper_3_New/Outputs/Feature_Engineering_v1/\n","\n","Loading and reshaping datasets...\n","Pivoting master dataset from long to wide format...\n","Calculating 'seabed_slope_to_target' from slope components.\n","Datasets loaded and reshaped successfully.\n","\n","--- Starting Feature Engineering Pipeline ---\n","\n","Creating lag features...\n","Creating rolling window features...\n","Creating cyclical time features...\n","Creating physics-informed interaction features...\n","--- Feature Engineering Pipeline Complete ---\n","\n","Cleaning training data: Removed 1149 rows with missing target.\n","\n","--- Starting Full Feature Selection Pipeline ---\n","[LightGBM] [Info] Total Bins 536557\n","[LightGBM] [Info] Number of data points in the train set: 10539, number of used features: 2114\n","[LightGBM] [Info] Start training from score 0.396068\n","Stage 1: Importance pruning removed 829 features.\n","\n","--- Starting Correlation-Based Feature Selection (Threshold=0.9) ---\n","Found and removed 941 features based on high correlation.\n","\n","Stage 3: Starting final model-based selection with 381 features remaining.\n","[LightGBM] [Info] Total Bins 96149\n","[LightGBM] [Info] Number of data points in the train set: 10539, number of used features: 381\n","[LightGBM] [Info] Start training from score 0.396068\n","\n","Final number of selected features: 202\n","--- Full Feature Selection Pipeline Complete ---\n","\n","Final cleaning: Removed 1983 rows from the combined dataset.\n","\n","--- Final Dataset Created ---\n","Shape of final dataset: (18470, 240)\n","Value counts for the final 'split' column:\n","split\n","Train_Val    10538\n","OOS           7932\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3-538169024.py:248: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_final.dropna(inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Success! Final engineered dataset saved to:\n","/content/drive/My Drive/Paper_3_New/Outputs/Feature_Engineering_v1/final_engineered_features_v3.csv\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Phase 1: Time-Series Feature Engineering for Coastal Wave Forecasting (Corrected v4.2)\n","\n","This script performs comprehensive feature engineering on a causally-optimized,\n","multivariate time series of coastal data. It prepares the dataset for a\n","subsequent predictive modeling phase.\n","\n","The pipeline includes:\n","1.  Setup and Data Loading from Google Drive, including a pivot from long to wide format.\n","2.  Efficient creation of lag, rolling window, cyclical, and interaction features.\n","3.  A corrected data splitting and cleaning workflow to prevent data leakage.\n","4.  A robust, multi-stage feature selection process optimized for speed and accuracy:\n","    a. Feature selection is performed *only* on the training/validation data.\n","    b. Importance-based pruning with LightGBM.\n","    c. Aggressive fast correlation-based pruning (threshold=0.90).\n","    d. Extremely Fast Final Selection using Scikit-learn's SelectFromModel.\n","5.  Creation and saving of the final engineered dataset.\n","\n","Author: Your Name/Gemini\n","Date: 2025-07-26\n","\"\"\"\n","\n","# =============================================================================\n","# Step 1: Setup and Environment Configuration\n","# =============================================================================\n","import os\n","import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.feature_selection import SelectFromModel\n","from google.colab import drive\n","\n","def setup_environment(project_root):\n","    \"\"\"\n","    Mounts Google Drive and sets up project paths.\n","    \"\"\"\n","    print(\"Mounting Google Drive...\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True)\n","        print(\"Google Drive mounted successfully.\")\n","    except Exception as e:\n","        print(f\"Error mounting Google Drive: {e}\")\n","        return None, None\n","\n","    input_data_path = os.path.join(project_root, 'Outputs/Predictor_Selection_v5_Physical/')\n","    # Update output folder to v3 to reflect the corrected methodology\n","    output_data_path = os.path.join(project_root, 'Outputs/Feature_Engineering_v1/')\n","\n","    os.makedirs(output_data_path, exist_ok=True)\n","    print(f\"Output directory created at: {output_data_path}\")\n","\n","    return input_data_path, output_data_path\n","\n","def load_data(input_path, static_path):\n","    \"\"\"\n","    Loads master time-series and static features. Reshapes the master dataset\n","    from long to wide format and engineers a slope feature.\n","    \"\"\"\n","    print(\"\\nLoading and reshaping datasets...\")\n","    try:\n","        df_long = pd.read_csv(input_path, parse_dates=['time'], index_col='time')\n","\n","        print(\"Pivoting master dataset from long to wide format...\")\n","        variables_to_pivot = ['hm0', 'tp', 'mdir', 'windspeed', 'winddirection', 'ssh']\n","        existing_vars = [var for var in variables_to_pivot if var in df_long.columns]\n","\n","        df_master = df_long.pivot_table(\n","            index='time', columns='point_id', values=existing_vars\n","        )\n","        df_master.columns = [f\"{col[1]}_{col[0]}\" for col in df_master.columns.values]\n","        df_master.columns = [col.replace('hm0', 'hs') for col in df_master.columns]\n","\n","        df_static = pd.read_csv(static_path)\n","\n","        if 'slope_ns' in df_static.columns and 'slope_ew' in df_static.columns:\n","            print(\"Calculating 'seabed_slope_to_target' from slope components.\")\n","            df_static['seabed_slope_to_target'] = np.sqrt(\n","                df_static['slope_ns'].fillna(0)**2 + df_static['slope_ew'].fillna(0)**2\n","            )\n","        else:\n","            print(\"Warning: Slope components not found. Setting 'seabed_slope_to_target' to 0.\")\n","            df_static['seabed_slope_to_target'] = 0\n","\n","        print(\"Datasets loaded and reshaped successfully.\")\n","        return df_master, df_static\n","    except FileNotFoundError as e:\n","        print(f\"Error loading data: {e}. Please check your file paths.\")\n","        return None, None\n","\n","# =============================================================================\n","# Step 2: Feature Engineering Pipeline\n","# =============================================================================\n","\n","def create_lag_features(df, lag_range=10):\n","    \"\"\"Creates lag features for all predictor columns efficiently.\"\"\"\n","    print(\"\\nCreating lag features...\")\n","    predictor_cols = [col for col in df.columns if col.startswith('offshore_')]\n","    lag_features = pd.concat(\n","        [df[col].shift(i).rename(f'{col}_lag_{i}h') for col in predictor_cols for i in range(1, lag_range + 1)],\n","        axis=1\n","    )\n","    return pd.concat([df, lag_features], axis=1)\n","\n","def create_rolling_features(df, windows=[3, 6, 12, 24]):\n","    \"\"\"Creates rolling window statistical features efficiently.\"\"\"\n","    print(\"Creating rolling window features...\")\n","    primary_predictors = [col for col in df.columns if '_hs' in col or '_tp' in col]\n","    rolling_features = []\n","    for col in primary_predictors:\n","        for window in windows:\n","            rolling_window = df[col].rolling(window=window, min_periods=1)\n","            rolling_features.append(rolling_window.mean().rename(f'{col}_roll_mean_{window}h'))\n","            rolling_features.append(rolling_window.std().rename(f'{col}_roll_std_{window}h'))\n","            rolling_features.append(rolling_window.min().rename(f'{col}_roll_min_{window}h'))\n","            rolling_features.append(rolling_window.max().rename(f'{col}_roll_max_{window}h'))\n","    return pd.concat([df] + rolling_features, axis=1)\n","\n","def create_cyclical_features(df):\n","    \"\"\"Creates cyclical features from the datetime index.\"\"\"\n","    print(\"Creating cyclical time features...\")\n","    df_cyclical = df.copy()\n","    df_cyclical['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 23.0)\n","    df_cyclical['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 23.0)\n","    df_cyclical['dayofyear_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365.0)\n","    df_cyclical['dayofyear_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365.0)\n","    return df_cyclical\n","\n","def create_interaction_features(df, df_static):\n","    \"\"\"Creates physics-informed interaction features.\"\"\"\n","    print(\"Creating physics-informed interaction features...\")\n","    df_interact = df.copy()\n","    offshore_locations = [pid for pid in df_static['point_id'].unique() if 'offshore' in pid]\n","\n","    for loc_id in offshore_locations:\n","        static_row = df_static[df_static['point_id'] == loc_id]\n","        if not static_row.empty:\n","            depth = static_row['depth'].iloc[0]\n","            slope = static_row['seabed_slope_to_target'].iloc[0]\n","            hs_col, tp_col = f'{loc_id}_hs', f'{loc_id}_tp'\n","            if hs_col in df_interact.columns:\n","                df_interact[f'{loc_id}_hs_x_depth'] = df_interact[hs_col] * depth\n","                df_interact[f'{loc_id}_hs_x_seabed_slope'] = df_interact[hs_col] * slope\n","            if tp_col in df_interact.columns:\n","                df_interact[f'{loc_id}_tp_x_depth'] = df_interact[tp_col] * depth\n","                df_interact[f'{loc_id}_tp_x_seabed_slope'] = df_interact[tp_col] * slope\n","    return df_interact\n","\n","def feature_engineering_pipeline(df_master, df_static):\n","    \"\"\"Main function to run the entire feature engineering pipeline.\"\"\"\n","    print(\"\\n--- Starting Feature Engineering Pipeline ---\")\n","    df_eng = create_lag_features(df_master)\n","    df_eng = create_rolling_features(df_eng)\n","    df_eng = create_cyclical_features(df_eng)\n","    df_eng = create_interaction_features(df_eng, df_static)\n","    print(\"--- Feature Engineering Pipeline Complete ---\")\n","    return df_eng\n","\n","# =============================================================================\n","# Step 3: Final Feature Selection (Optimized v4 - High Speed)\n","# =============================================================================\n","\n","def select_features_by_correlation(X, threshold=0.90):\n","    \"\"\"Removes highly correlated features as a fast pre-filtering step.\"\"\"\n","    print(f\"\\n--- Starting Correlation-Based Feature Selection (Threshold={threshold}) ---\")\n","    corr_matrix = X.corr().abs()\n","    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n","    print(f\"Found and removed {len(to_drop)} features based on high correlation.\")\n","    return X.drop(columns=to_drop).columns.tolist()\n","\n","def feature_selection_pipeline(X, y, correlation_threshold=0.90):\n","    \"\"\"\n","    Runs the full, optimized feature selection pipeline using a fast, model-based final step.\n","    \"\"\"\n","    print(\"\\n--- Starting Full Feature Selection Pipeline ---\")\n","\n","    # 1. Importance-Based Pruning (Quick First Pass)\n","    lgbm = lgb.LGBMRegressor(random_state=42, force_col_wise=True)\n","    lgbm.fit(X, y)\n","    importances = pd.Series(lgbm.feature_importances_, index=X.columns)\n","    important_features = importances[importances > 0].index.tolist()\n","    print(f\"Stage 1: Importance pruning removed {X.shape[1] - len(important_features)} features.\")\n","    X_important = X[important_features]\n","\n","    # 2. Fast Correlation-Based Pruning (Aggressive Bulk Removal)\n","    correlated_pruned_features = select_features_by_correlation(X_important, threshold=correlation_threshold)\n","    X_correlated_pruned = X_important[correlated_pruned_features]\n","\n","    # 3. Final Selection using SelectFromModel (Extremely Fast & Robust)\n","    print(f\"\\nStage 3: Starting final model-based selection with {X_correlated_pruned.shape[1]} features remaining.\")\n","    selector_model = lgb.LGBMRegressor(random_state=42, force_col_wise=True)\n","    selector = SelectFromModel(selector_model, prefit=False, threshold='median')\n","    selector.fit(X_correlated_pruned, y)\n","    final_features = X_correlated_pruned.columns[selector.get_support()].tolist()\n","\n","    print(f\"\\nFinal number of selected features: {len(final_features)}\")\n","    print(\"--- Full Feature Selection Pipeline Complete ---\")\n","    return final_features\n","\n","# =============================================================================\n","# Main Execution Block (Corrected Workflow)\n","# =============================================================================\n","if __name__ == '__main__':\n","    PROJECT_ROOT = '/content/drive/My Drive/Paper_3_New/'\n","    SPLIT_DATE = '2022-01-01'\n","\n","    input_dir, output_dir = setup_environment(PROJECT_ROOT)\n","\n","    if input_dir and output_dir:\n","        master_file_path = os.path.join(input_dir, 'master_dataset_causally_optimized_v2.csv')\n","        static_file_path = os.path.join(input_dir, 'static_features.csv')\n","\n","        df_master, df_static = load_data(master_file_path, static_file_path)\n","\n","        if df_master is not None and df_static is not None:\n","            # 1. Create all features (introduces NaNs)\n","            df_engineered = feature_engineering_pipeline(df_master, df_static)\n","\n","            # 2. Add the split column BEFORE any data is dropped\n","            df_engineered['split'] = np.where(df_engineered.index >= pd.to_datetime(SPLIT_DATE), 'OOS', 'Train_Val')\n","\n","            # 3. Prepare data for feature selection using ONLY Train_Val data\n","            df_train_val = df_engineered[df_engineered['split'] == 'Train_Val'].copy()\n","\n","            # 4. Smart NaN Removal: Only drop rows in the training set where the target is missing\n","            # This is the essential cleaning step before training.\n","            initial_train_rows = df_train_val.shape[0]\n","            df_train_val.dropna(subset=['buoy_main_hs'], inplace=True)\n","            print(f\"\\nCleaning training data: Removed {initial_train_rows - df_train_val.shape[0]} rows with missing target.\")\n","\n","            target_cols = [col for col in df_train_val.columns if col.startswith('buoy_main_')]\n","            feature_cols = [col for col in df_train_val.columns if col not in target_cols and col != 'split']\n","\n","            X_train = df_train_val[feature_cols]\n","            y_train = df_train_val['buoy_main_hs']\n","\n","            # 5. Run the selection pipeline on the CLEANED training data only\n","            final_feature_list = feature_selection_pipeline(X_train, y_train)\n","\n","            # 6. Create the final dataset using the selected features on the ENTIRE dataframe\n","            final_cols = final_feature_list + target_cols + ['split']\n","            df_final = df_engineered[final_cols].copy()\n","            # 7. Final Cleaning: Drop any remaining NaNs from the whole dataset\n","            # This primarily removes the initial rows in the OOS set affected by lags/rolling windows.\n","            initial_total_rows = df_final.shape[0]\n","            df_final.dropna(inplace=True)\n","            print(f\"\\nFinal cleaning: Removed {initial_total_rows - df_final.shape[0]} rows from the combined dataset.\")\n","\n","            print(\"\\n--- Final Dataset Created ---\")\n","            print(f\"Shape of final dataset: {df_final.shape}\")\n","            print(\"Value counts for the final 'split' column:\")\n","            print(df_final['split'].value_counts())\n","\n","            # 8. Save the final dataset\n","            output_file_path = os.path.join(output_dir, 'final_engineered_features_v3.csv')\n","            df_final.to_csv(output_file_path)\n","\n","            print(f\"\\nâœ… Success! Final engineered dataset saved to:\\n{output_file_path}\")\n"]}]}